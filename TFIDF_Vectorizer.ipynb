{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE FOR TASK1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataset):\n",
    "    '''\n",
    "    Makes vocabulary from a corpus\n",
    "    '''\n",
    "    unique_words = set()   #For keeping only unique words\n",
    "    if isinstance(dataset, (list,)):  #Checks if the given corpus is in the format of list or not\n",
    "        for row in dataset:           #For each document in the dataset\n",
    "            for word in row.split():  #Creates a list of words in the document\n",
    "                if len(word) < 2:     \n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        vocab = {j : i for i, j in enumerate(sorted(list(unique_words)))} #Creates a dictionary of all unique words\n",
    "        return vocab\n",
    "    else:\n",
    "         print('You need to pass a list of sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n"
     ]
    }
   ],
   "source": [
    "corpus = ['this is the first document',\n",
    "          'this document is the second document',\n",
    "          'and this is the third one',\n",
    "          'is this the first document']\n",
    "vocab = fit(corpus)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DocumentLength(doc):\n",
    "    '''\n",
    "    Computes the length of the given document\n",
    "    '''\n",
    "    doc_length = 0\n",
    "    for ele in doc:  \n",
    "        doc_length += 1\n",
    "    return doc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordOccurrence(doc, word):\n",
    "    '''\n",
    "    Computes the number of times the given term occurred in the given document\n",
    "    '''\n",
    "    word_frequency = 0\n",
    "    for ele in doc:\n",
    "        if ele == word:\n",
    "            word_frequency += 1\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TotalDocumentNumber(dataset):\n",
    "    '''\n",
    "    Count the number of documents in the given corpus\n",
    "    '''\n",
    "    total_document = 0\n",
    "    for doc in dataset:\n",
    "        total_document += 1\n",
    "    return total_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DocumentWithTerm(dataset, word):\n",
    "    '''\n",
    "    Count the number of documents which has the given term in it\n",
    "    '''\n",
    "    docs_with_term = 0\n",
    "    for doc in dataset:\n",
    "        for doc_word in doc.split():\n",
    "            if doc_word == word:\n",
    "                docs_with_term += 1\n",
    "                break\n",
    "    return docs_with_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset):\n",
    "    '''\n",
    "    Transforms the given corpus into TFIDF vectorizer\n",
    "    '''\n",
    "    if isinstance(dataset, (list,)):  #checks if the given corpus is in the format of list or not\n",
    "        rows = []     \n",
    "        columns = []\n",
    "        values = []\n",
    "        IDF = []\n",
    "        #vocab contains the unique words and the dimension of each word in the format of dictionary where the key\n",
    "        #represents the unique word and the value of the key is the dimension of that word\n",
    "        vocab = fit(dataset)   \n",
    "        #total_documents holds the number of documents in the corpus\n",
    "        total_documents = TotalDocumentNumber(dataset)\n",
    "        for word in vocab:   \n",
    "            IDF.append(1 + np.log((1 + total_documents) / (1 + DocumentWithTerm(dataset, word)))) #Computes the\n",
    "            #IDF value of the each unique word in the vocabulary \n",
    "        for idx, doc in enumerate(tqdm(dataset)):\n",
    "            for word in list(set(doc.split())): #Splits the document which is in the format of string and makes \n",
    "                #a list of unique words in the document\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                for key in vocab: #Iterates through the each key in the vocabulary\n",
    "                    if key == word: #If the matching key is found then the dimension of the word is stored in the \n",
    "                        column_index = vocab[key] #column index\n",
    "                        break\n",
    "                rows.append(idx)\n",
    "                columns.append(column_index)\n",
    "                #tf holds the tf value of a term in the current document\n",
    "                tf = WordOccurrence(doc.split(), word) / DocumentLength(doc.split())\n",
    "                values.append(tf * IDF[column_index])\n",
    "        return csr_matrix((values, (rows, columns)), shape = (len(dataset), len(vocab)))\n",
    "    else:\n",
    "        print('You need to pass a list of sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 1284.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf = transform(corpus)  #tfidf holds the tfidf values of the documents in the corpus\n",
    "tfidfl2 = normalize(tfidf) #tfidfl2 holds the L2 normalize values of the documents in the corpus \n",
    "print(tfidfl2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "#For comparing result with the sklearn implementation of the TFIDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(skl_output.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidfl2.toarray()) #prints the tfidf values of all the documents in the dense form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE FOR TASK2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UniqueWords(dataset):\n",
    "    '''\n",
    "    Counts unique words in the given corpus\n",
    "    '''\n",
    "    unique_words = set()\n",
    "    for doc in dataset:   #To iterate through each document\n",
    "        for word in doc.split():  #To iterate through each term in the document\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            unique_words.add(word)\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DocumentLength(doc):\n",
    "    '''\n",
    "    Finds the length of the given document\n",
    "    '''\n",
    "    doc_length = 0\n",
    "    for word in doc.split():   #To iterate through each term in the document\n",
    "        doc_length += 1\n",
    "    return doc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TotalDocumentNumber(dataset):\n",
    "    '''\n",
    "    Counts the total document in the given corpus\n",
    "    '''\n",
    "    doc_number = 0\n",
    "    for doc in dataset:  #To iterate through each document in the corpus\n",
    "        doc_number += 1\n",
    "    return doc_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordOccurrence(doc, word):\n",
    "    '''\n",
    "    Counts the occurrences of the given term in the given document\n",
    "    '''\n",
    "    word_occ = 0\n",
    "    for ele in doc.split(): #To iterate through each term in the document\n",
    "        if ele == word:\n",
    "            word_occ += 1\n",
    "    return word_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DocumentWithTerm(dataset, word):\n",
    "    '''\n",
    "    Counts the documents which have the given term\n",
    "    '''\n",
    "    docs_with_term = 0\n",
    "    for doc in dataset:  #To iterate through each document in the corpus\n",
    "        for ele in doc.split(): #To iterate through each term in a document\n",
    "            if ele == word:\n",
    "                docs_with_term += 1\n",
    "                break\n",
    "    return docs_with_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataset):\n",
    "    '''\n",
    "    Constructs the vocabulary of the top 50 terms based on their idf values \n",
    "    '''\n",
    "    if isinstance(dataset, (list,)):\n",
    "        unique_words = UniqueWords(dataset)\n",
    "        IDF = []\n",
    "        temp_idf = []\n",
    "        #At the end of the for loop we get a list named IDF containing idf values of all unique words\n",
    "        for word in list(unique_words):\n",
    "            lst = []\n",
    "            idf = 1 + np.log((1 + TotalDocumentNumber(dataset)) / (1 + DocumentWithTerm(dataset,word)))\n",
    "            lst.append(word)\n",
    "            lst.append(idf)\n",
    "            IDF.append(lst)\n",
    "        lst = []\n",
    "        for ele in IDF:\n",
    "            lst.append(ele[1])  #Taking only idf values in the list lst\n",
    "        lst = sorted(lst, reverse = True)  #Sorting the list lst in decreasing order of idf values\n",
    "        for i in range(0, 50):   #To iterate only through top 50 idf values\n",
    "            for j in range(0, len(IDF)):  #To iterate through the list of idf values \n",
    "                if lst[i] == IDF[j][1]:   #To check if the idf value of a term is in the list of top 50 idf values\n",
    "                    temp_idf.append(IDF[j]) #If it is,then append the value in the list temp_idf\n",
    "                    IDF.pop(j) #Here we are popping the term whose idf value is in the list of top 50 values, otherwise \n",
    "                    break      #we may get the idf value of a term many times in the list temp_idf if some idf values are equal in list IDF\n",
    "        vocab = {temp_idf[i][0] : i for i in range(0, 50)} #To constructs the vocabulary of the top 50 terms according to the idf values\n",
    "        return vocab, temp_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary of the top  50 terms based on their idf values\n",
      "{'views': 0, 'lid': 1, 'lestat': 2, 'represents': 3, 'george': 4, 'rough': 5, 'spacey': 6, 'redeemed': 7, 'scripted': 8, 'mystifying': 9, 'rumbles': 10, 'tender': 11, 'conrad': 12, 'seat': 13, 'thanks': 14, 'affected': 15, 'empowerment': 16, 'iffy': 17, 'guys': 18, 'chimp': 19, 'judging': 20, 'cases': 21, 'cry': 22, 'faultless': 23, 'weaving': 24, 'rpg': 25, 'celebrity': 26, 'traditional': 27, 'bond': 28, 'artistic': 29, 'value': 30, 'juano': 31, 'embassy': 32, 'fifties': 33, 'decisions': 34, 'greatness': 35, 'dysfunction': 36, 'strange': 37, 'messages': 38, 'bakery': 39, 'elderly': 40, 'random': 41, 'stuart': 42, 'boogeyman': 43, 'fare': 44, 'tear': 45, 'ben': 46, 'oriented': 47, 'cuts': 48, 'distressed': 49}\n"
     ]
    }
   ],
   "source": [
    "vocab, IDF = fit(corpus)\n",
    "print('The vocabulary of the top  50 terms based on their idf values')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 50 idf values\n",
      "[['views', 6.922918004572872], ['lid', 6.922918004572872], ['lestat', 6.922918004572872], ['represents', 6.922918004572872], ['george', 6.922918004572872], ['rough', 6.922918004572872], ['spacey', 6.922918004572872], ['redeemed', 6.922918004572872], ['scripted', 6.922918004572872], ['mystifying', 6.922918004572872], ['rumbles', 6.922918004572872], ['tender', 6.922918004572872], ['conrad', 6.922918004572872], ['seat', 6.922918004572872], ['thanks', 6.922918004572872], ['affected', 6.922918004572872], ['empowerment', 6.922918004572872], ['iffy', 6.922918004572872], ['guys', 6.922918004572872], ['chimp', 6.922918004572872], ['judging', 6.922918004572872], ['cases', 6.922918004572872], ['cry', 6.922918004572872], ['faultless', 6.922918004572872], ['weaving', 6.922918004572872], ['rpg', 6.922918004572872], ['celebrity', 6.922918004572872], ['traditional', 6.922918004572872], ['bond', 6.922918004572872], ['artistic', 6.922918004572872], ['value', 6.922918004572872], ['juano', 6.922918004572872], ['embassy', 6.922918004572872], ['fifties', 6.922918004572872], ['decisions', 6.922918004572872], ['greatness', 6.922918004572872], ['dysfunction', 6.922918004572872], ['strange', 6.922918004572872], ['messages', 6.922918004572872], ['bakery', 6.922918004572872], ['elderly', 6.922918004572872], ['random', 6.922918004572872], ['stuart', 6.922918004572872], ['boogeyman', 6.922918004572872], ['fare', 6.922918004572872], ['tear', 6.922918004572872], ['ben', 6.922918004572872], ['oriented', 6.922918004572872], ['cuts', 6.922918004572872], ['distressed', 6.922918004572872]]\n"
     ]
    }
   ],
   "source": [
    "print('The top 50 idf values')\n",
    "print(IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset):\n",
    "    '''\n",
    "    Transforms the given corpus into TFIDF vectorizer containing only the top 50 terms based on their idf values \n",
    "    '''\n",
    "    if isinstance(dataset, (list,)): #To check whether the given corpus is in the form of list\n",
    "        rows = []\n",
    "        columns = []\n",
    "        values = []\n",
    "        vocab, IDF = fit(dataset)   #vocab contains the vocabulary of the top 50 terms based on their idf values, and IDF contains the idf values of the corresponding term  in the vocabulary\n",
    "        totaldocument = TotalDocumentNumber(dataset)\n",
    "        for idx, doc in enumerate(tqdm(dataset)):\n",
    "            for term in list(set(doc.split())):  #Splits the document which is in the format of string, and makes a list of all unique words\n",
    "                if len(term) < 2:\n",
    "                    continue\n",
    "                for key in vocab:  #At the end of this for loop we get tfidf values of the top 50 terms based on their idf values\n",
    "                    #in the list 'values',and the lists 'rows' and 'columns' contain the corresponding row and column values of the term which is in the vocabulary\n",
    "                    if key == term:\n",
    "                        column_index = vocab[key]\n",
    "                        rows.append(idx)\n",
    "                        columns.append(column_index)\n",
    "                        tf = WordOccurrence(doc, term) / DocumentLength(doc)\n",
    "                        values.append(tf * IDF[column_index][1])\n",
    "                        break\n",
    "        return csr_matrix((values, (rows, columns)), shape = (len(dataset), len(vocab)))  #transform function returns the tfidf vectorizer in the form of sparse matrix\n",
    "    else:\n",
    "        print('You need to pass a list of documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 746/746 [00:00<00:00, 21438.81it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf = transform(corpus) #tfidf contains the sparse matrix of the corpus\n",
    "tfidfl2 = normalize(tfidf) #tfidfl2 contains the l2 normalized sparse matrix of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The l2 normalized top 50 tfidf values in the form of sparse matrix\n",
      "  (0, 49)\t1.0\n",
      "  (7, 38)\t1.0\n",
      "  (19, 44)\t1.0\n",
      "  (60, 13)\t1.0\n",
      "  (68, 46)\t1.0\n",
      "  (75, 21)\t1.0\n",
      "  (80, 22)\t1.0\n",
      "  (87, 7)\t1.0\n",
      "  (106, 16)\t1.0\n",
      "  (109, 43)\t1.0\n",
      "  (135, 10)\t0.5\n",
      "  (135, 14)\t0.5\n",
      "  (135, 33)\t0.5\n",
      "  (135, 35)\t0.5\n",
      "  (137, 29)\t1.0\n",
      "  (148, 39)\t0.7071067811865476\n",
      "  (148, 45)\t0.7071067811865476\n",
      "  (161, 28)\t1.0\n",
      "  (190, 23)\t1.0\n",
      "  (193, 15)\t1.0\n",
      "  (205, 3)\t1.0\n",
      "  (222, 1)\t1.0\n",
      "  (238, 19)\t1.0\n",
      "  (254, 30)\t1.0\n",
      "  (277, 40)\t1.0\n",
      "  (305, 0)\t1.0\n",
      "  (310, 5)\t1.0\n",
      "  (350, 12)\t1.0\n",
      "  (396, 11)\t0.7071067811865476\n",
      "  (396, 47)\t0.7071067811865476\n",
      "  (434, 4)\t1.0\n",
      "  (436, 41)\t1.0\n",
      "  (437, 32)\t1.0\n",
      "  (447, 17)\t1.0\n",
      "  (475, 20)\t1.0\n",
      "  (516, 25)\t1.0\n",
      "  (517, 48)\t1.0\n",
      "  (547, 2)\t0.7071067811865476\n",
      "  (547, 42)\t0.7071067811865476\n",
      "  (550, 36)\t1.0\n",
      "  (572, 37)\t1.0\n",
      "  (619, 8)\t1.0\n",
      "  (628, 31)\t1.0\n",
      "  (644, 6)\t0.5773502691896257\n",
      "  (644, 26)\t0.5773502691896257\n",
      "  (644, 34)\t0.5773502691896257\n",
      "  (652, 9)\t1.0\n",
      "  (665, 18)\t1.0\n",
      "  (715, 27)\t1.0\n",
      "  (734, 24)\t1.0\n",
      "The l2 normalized top 50 tfidf values in the form of dense matrix\n",
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('The l2 normalized top 50 tfidf values in the form of sparse matrix')\n",
    "print(tfidfl2)\n",
    "print('The l2 normalized top 50 tfidf values in the form of dense matrix')\n",
    "print(tfidfl2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
